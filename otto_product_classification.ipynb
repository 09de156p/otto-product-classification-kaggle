{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c3c84f-6e5e-4a55-a9dc-0810ef0fd4d1",
   "metadata": {},
   "source": [
    "# 多クラス分類の学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c637287-2905-4cba-824b-d4dcdb96ca29",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f055d03-9dc7-4ed3-a65d-151c21d9fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a412771c-cf8b-4d3d-a474-bde1aeccb128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b63c1311-2a68-4dc2-89fc-381ce228c06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, ..., 0, 0, 'Class_1'],\n",
       "       [2, 0, 0, ..., 0, 0, 'Class_1'],\n",
       "       [3, 0, 0, ..., 0, 0, 'Class_1'],\n",
       "       ...,\n",
       "       [61876, 0, 0, ..., 0, 0, 'Class_9'],\n",
       "       [61877, 1, 0, ..., 10, 0, 'Class_9'],\n",
       "       [61878, 0, 0, ..., 2, 0, 'Class_9']], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\", decimal=\",\")\n",
    "ds = data.values\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "943beca7-e648-4bb5-ad8e-c2e069f7227f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61873</th>\n",
       "      <td>61874</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61874</th>\n",
       "      <td>61875</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61875</th>\n",
       "      <td>61876</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61876</th>\n",
       "      <td>61877</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61877</th>\n",
       "      <td>61878</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61878 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "0          1       1       0       0       0       0       0       0       0   \n",
       "1          2       0       0       0       0       0       0       0       1   \n",
       "2          3       0       0       0       0       0       0       0       1   \n",
       "3          4       1       0       0       1       6       1       5       0   \n",
       "4          5       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "61873  61874       1       0       0       1       1       0       0       0   \n",
       "61874  61875       4       0       0       0       0       0       0       0   \n",
       "61875  61876       0       0       0       0       0       0       0       3   \n",
       "61876  61877       1       0       0       0       0       0       0       0   \n",
       "61877  61878       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       feat_9  ...  feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  \\\n",
       "0           0  ...        1        0        0        0        0        0   \n",
       "1           0  ...        0        0        0        0        0        0   \n",
       "2           0  ...        0        0        0        0        0        0   \n",
       "3           0  ...        0        1        2        0        0        0   \n",
       "4           0  ...        1        0        0        0        0        1   \n",
       "...       ...  ...      ...      ...      ...      ...      ...      ...   \n",
       "61873       0  ...        1        0        0        0        0        0   \n",
       "61874       0  ...        0        2        0        0        2        0   \n",
       "61875       1  ...        0        3        1        0        0        0   \n",
       "61876       0  ...        0        0        0        0        1        0   \n",
       "61877       0  ...        0        0        0        0        0        0   \n",
       "\n",
       "       feat_91  feat_92  feat_93   target  \n",
       "0            0        0        0  Class_1  \n",
       "1            0        0        0  Class_1  \n",
       "2            0        0        0  Class_1  \n",
       "3            0        0        0  Class_1  \n",
       "4            0        0        0  Class_1  \n",
       "...        ...      ...      ...      ...  \n",
       "61873        0        2        0  Class_9  \n",
       "61874        0        1        0  Class_9  \n",
       "61875        0        0        0  Class_9  \n",
       "61876        3       10        0  Class_9  \n",
       "61877        0        2        0  Class_9  \n",
       "\n",
       "[61878 rows x 95 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed4a1b81-a336-42db-819b-e1b99b8606d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.00000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Class_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30939.500000</td>\n",
       "      <td>0.38668</td>\n",
       "      <td>0.263066</td>\n",
       "      <td>0.901467</td>\n",
       "      <td>0.779081</td>\n",
       "      <td>0.071043</td>\n",
       "      <td>0.025696</td>\n",
       "      <td>0.193704</td>\n",
       "      <td>0.662433</td>\n",
       "      <td>1.011296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.532306</td>\n",
       "      <td>1.128576</td>\n",
       "      <td>0.393549</td>\n",
       "      <td>0.874915</td>\n",
       "      <td>0.457772</td>\n",
       "      <td>0.812421</td>\n",
       "      <td>0.264941</td>\n",
       "      <td>0.380119</td>\n",
       "      <td>0.126135</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17862.784315</td>\n",
       "      <td>1.52533</td>\n",
       "      <td>1.252073</td>\n",
       "      <td>2.934818</td>\n",
       "      <td>2.788005</td>\n",
       "      <td>0.438902</td>\n",
       "      <td>0.215333</td>\n",
       "      <td>1.030102</td>\n",
       "      <td>2.255770</td>\n",
       "      <td>3.474822</td>\n",
       "      <td>...</td>\n",
       "      <td>1.900438</td>\n",
       "      <td>2.681554</td>\n",
       "      <td>1.575455</td>\n",
       "      <td>2.115466</td>\n",
       "      <td>1.527385</td>\n",
       "      <td>4.597804</td>\n",
       "      <td>2.045646</td>\n",
       "      <td>0.982385</td>\n",
       "      <td>1.201720</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15470.250000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30939.500000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>46408.750000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61.00000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id       feat_1        feat_2        feat_3        feat_4  \\\n",
       "count   61878.000000  61878.00000  61878.000000  61878.000000  61878.000000   \n",
       "unique           NaN          NaN           NaN           NaN           NaN   \n",
       "top              NaN          NaN           NaN           NaN           NaN   \n",
       "freq             NaN          NaN           NaN           NaN           NaN   \n",
       "mean    30939.500000      0.38668      0.263066      0.901467      0.779081   \n",
       "std     17862.784315      1.52533      1.252073      2.934818      2.788005   \n",
       "min         1.000000      0.00000      0.000000      0.000000      0.000000   \n",
       "25%     15470.250000      0.00000      0.000000      0.000000      0.000000   \n",
       "50%     30939.500000      0.00000      0.000000      0.000000      0.000000   \n",
       "75%     46408.750000      0.00000      0.000000      0.000000      0.000000   \n",
       "max     61878.000000     61.00000     51.000000     64.000000     70.000000   \n",
       "\n",
       "              feat_5        feat_6        feat_7        feat_8        feat_9  \\\n",
       "count   61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean        0.071043      0.025696      0.193704      0.662433      1.011296   \n",
       "std         0.438902      0.215333      1.030102      2.255770      3.474822   \n",
       "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "max        19.000000     10.000000     38.000000     76.000000     43.000000   \n",
       "\n",
       "        ...       feat_85       feat_86       feat_87       feat_88  \\\n",
       "count   ...  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "unique  ...           NaN           NaN           NaN           NaN   \n",
       "top     ...           NaN           NaN           NaN           NaN   \n",
       "freq    ...           NaN           NaN           NaN           NaN   \n",
       "mean    ...      0.532306      1.128576      0.393549      0.874915   \n",
       "std     ...      1.900438      2.681554      1.575455      2.115466   \n",
       "min     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%     ...      0.000000      1.000000      0.000000      1.000000   \n",
       "max     ...     55.000000     65.000000     67.000000     30.000000   \n",
       "\n",
       "             feat_89       feat_90       feat_91       feat_92       feat_93  \\\n",
       "count   61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean        0.457772      0.812421      0.264941      0.380119      0.126135   \n",
       "std         1.527385      4.597804      2.045646      0.982385      1.201720   \n",
       "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        61.000000    130.000000     52.000000     19.000000     87.000000   \n",
       "\n",
       "         target  \n",
       "count     61878  \n",
       "unique        9  \n",
       "top     Class_2  \n",
       "freq      16122  \n",
       "mean        NaN  \n",
       "std         NaN  \n",
       "min         NaN  \n",
       "25%         NaN  \n",
       "50%         NaN  \n",
       "75%         NaN  \n",
       "max         NaN  \n",
       "\n",
       "[11 rows x 95 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cc8a01f-cb42-4556-945a-427a94cc58cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_84</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144363</th>\n",
       "      <td>144364</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144364</th>\n",
       "      <td>144365</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144365</th>\n",
       "      <td>144366</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144366</th>\n",
       "      <td>144367</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144367</th>\n",
       "      <td>144368</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144368 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  \\\n",
       "0            1       0       0       0       0       0       0       0   \n",
       "1            2       2       2      14      16       0       0       0   \n",
       "2            3       0       1      12       1       0       0       0   \n",
       "3            4       0       0       0       1       0       0       0   \n",
       "4            5       1       0       0       1       0       0       1   \n",
       "...        ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "144363  144364       0       0       0       0       0       0       0   \n",
       "144364  144365       0       0       0       0       0       0       0   \n",
       "144365  144366       0       1       0       0       0       0       1   \n",
       "144366  144367       0       0       0       0       0       0       0   \n",
       "144367  144368       0       0       0       0       0       0       0   \n",
       "\n",
       "        feat_8  feat_9  ...  feat_84  feat_85  feat_86  feat_87  feat_88  \\\n",
       "0            0       0  ...        0        0       11        1       20   \n",
       "1            0       0  ...        0        0        0        0        0   \n",
       "2            0       0  ...        0        0        0        0        2   \n",
       "3            0       0  ...        0        3        1        0        0   \n",
       "4            2       0  ...        0        0        0        0        0   \n",
       "...        ...     ...  ...      ...      ...      ...      ...      ...   \n",
       "144363       0       0  ...        0        0        2        1        1   \n",
       "144364       0       0  ...        0        1        4        1       11   \n",
       "144365       1       0  ...        0        1        3        1        1   \n",
       "144366       0       0  ...        0        0        0        0        5   \n",
       "144367       0       0  ...        0        0        9        1        6   \n",
       "\n",
       "        feat_89  feat_90  feat_91  feat_92  feat_93  \n",
       "0             0        0        0        0        0  \n",
       "1             4        0        0        2        0  \n",
       "2             0        0        0        0        1  \n",
       "3             0        0        0        0        0  \n",
       "4             0        0        9        0        0  \n",
       "...         ...      ...      ...      ...      ...  \n",
       "144363        0        0        0        0        0  \n",
       "144364        0        0        0        0        0  \n",
       "144365        0        0        1        0        0  \n",
       "144366        0        0        0        1        0  \n",
       "144367        0        0        0        0        0  \n",
       "\n",
       "[144368 rows x 94 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"test.csv\", decimal=\",\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73d78f2d-2721-418f-84ca-a9fa136448cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trdata = ds[:, 1:94]\n",
    "y_trdata = ds[:, 94]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82b6ac54-36e3-42b4-ac53-a52e2b22acd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 8, 8, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_trdata = le.fit_transform(y_trdata).astype(int)\n",
    "y_trdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c28e27c2-b68c-4336-baff-92323dc1558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 326\n",
    "ts = 0.20\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=ts, random_state=seed)\n",
    "for train_idx, test_idx in sss.split(x_trdata, y_trdata):\n",
    "    x_train = x_trdata[train_idx]\n",
    "    y_train = y_trdata[train_idx]\n",
    "    x_val = x_trdata[test_idx]\n",
    "    y_val = y_trdata[test_idx]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edcefc7e-9cfd-4c4f-8668-f71bf3ac1855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "430faa0d-b634-4dfb-baa1-d349fc7728f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"target\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98746c06-b9de-4267-ac12-f98058f32469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>feat_11</th>\n",
       "      <th>feat_12</th>\n",
       "      <th>feat_13</th>\n",
       "      <th>feat_14</th>\n",
       "      <th>feat_15</th>\n",
       "      <th>feat_16</th>\n",
       "      <th>feat_17</th>\n",
       "      <th>feat_18</th>\n",
       "      <th>feat_19</th>\n",
       "      <th>feat_20</th>\n",
       "      <th>feat_21</th>\n",
       "      <th>feat_22</th>\n",
       "      <th>feat_23</th>\n",
       "      <th>feat_24</th>\n",
       "      <th>feat_25</th>\n",
       "      <th>feat_26</th>\n",
       "      <th>feat_27</th>\n",
       "      <th>feat_28</th>\n",
       "      <th>feat_29</th>\n",
       "      <th>feat_30</th>\n",
       "      <th>feat_31</th>\n",
       "      <th>feat_32</th>\n",
       "      <th>feat_33</th>\n",
       "      <th>feat_34</th>\n",
       "      <th>feat_35</th>\n",
       "      <th>feat_36</th>\n",
       "      <th>feat_37</th>\n",
       "      <th>feat_38</th>\n",
       "      <th>feat_39</th>\n",
       "      <th>feat_40</th>\n",
       "      <th>feat_41</th>\n",
       "      <th>feat_42</th>\n",
       "      <th>feat_43</th>\n",
       "      <th>feat_44</th>\n",
       "      <th>feat_45</th>\n",
       "      <th>feat_46</th>\n",
       "      <th>feat_47</th>\n",
       "      <th>feat_48</th>\n",
       "      <th>feat_49</th>\n",
       "      <th>feat_50</th>\n",
       "      <th>feat_51</th>\n",
       "      <th>feat_52</th>\n",
       "      <th>feat_53</th>\n",
       "      <th>feat_54</th>\n",
       "      <th>feat_55</th>\n",
       "      <th>feat_56</th>\n",
       "      <th>feat_57</th>\n",
       "      <th>feat_58</th>\n",
       "      <th>feat_59</th>\n",
       "      <th>feat_60</th>\n",
       "      <th>feat_61</th>\n",
       "      <th>feat_62</th>\n",
       "      <th>feat_63</th>\n",
       "      <th>feat_64</th>\n",
       "      <th>feat_65</th>\n",
       "      <th>feat_66</th>\n",
       "      <th>feat_67</th>\n",
       "      <th>feat_68</th>\n",
       "      <th>feat_69</th>\n",
       "      <th>feat_70</th>\n",
       "      <th>feat_71</th>\n",
       "      <th>feat_72</th>\n",
       "      <th>feat_73</th>\n",
       "      <th>feat_74</th>\n",
       "      <th>feat_75</th>\n",
       "      <th>feat_76</th>\n",
       "      <th>feat_77</th>\n",
       "      <th>feat_78</th>\n",
       "      <th>feat_79</th>\n",
       "      <th>feat_80</th>\n",
       "      <th>feat_81</th>\n",
       "      <th>feat_82</th>\n",
       "      <th>feat_83</th>\n",
       "      <th>feat_84</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.00000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30939.500000</td>\n",
       "      <td>0.38668</td>\n",
       "      <td>0.263066</td>\n",
       "      <td>0.901467</td>\n",
       "      <td>0.779081</td>\n",
       "      <td>0.071043</td>\n",
       "      <td>0.025696</td>\n",
       "      <td>0.193704</td>\n",
       "      <td>0.662433</td>\n",
       "      <td>1.011296</td>\n",
       "      <td>0.263906</td>\n",
       "      <td>1.252869</td>\n",
       "      <td>0.140874</td>\n",
       "      <td>0.480979</td>\n",
       "      <td>1.696693</td>\n",
       "      <td>1.284398</td>\n",
       "      <td>1.413459</td>\n",
       "      <td>0.366108</td>\n",
       "      <td>0.575423</td>\n",
       "      <td>0.551699</td>\n",
       "      <td>0.471525</td>\n",
       "      <td>0.204014</td>\n",
       "      <td>0.729969</td>\n",
       "      <td>0.142522</td>\n",
       "      <td>2.643880</td>\n",
       "      <td>1.534520</td>\n",
       "      <td>0.563108</td>\n",
       "      <td>0.696613</td>\n",
       "      <td>0.238970</td>\n",
       "      <td>0.275768</td>\n",
       "      <td>0.150312</td>\n",
       "      <td>0.148680</td>\n",
       "      <td>1.043796</td>\n",
       "      <td>0.696516</td>\n",
       "      <td>0.946411</td>\n",
       "      <td>0.666263</td>\n",
       "      <td>0.709089</td>\n",
       "      <td>0.263632</td>\n",
       "      <td>0.582129</td>\n",
       "      <td>0.485585</td>\n",
       "      <td>1.653059</td>\n",
       "      <td>0.303468</td>\n",
       "      <td>0.698019</td>\n",
       "      <td>0.451146</td>\n",
       "      <td>0.560829</td>\n",
       "      <td>0.238130</td>\n",
       "      <td>0.641375</td>\n",
       "      <td>0.249669</td>\n",
       "      <td>1.584893</td>\n",
       "      <td>0.348314</td>\n",
       "      <td>0.324283</td>\n",
       "      <td>0.053298</td>\n",
       "      <td>0.213485</td>\n",
       "      <td>0.442063</td>\n",
       "      <td>2.072465</td>\n",
       "      <td>0.323120</td>\n",
       "      <td>0.303775</td>\n",
       "      <td>0.309108</td>\n",
       "      <td>0.697970</td>\n",
       "      <td>0.388603</td>\n",
       "      <td>1.029930</td>\n",
       "      <td>0.239746</td>\n",
       "      <td>1.187563</td>\n",
       "      <td>0.168590</td>\n",
       "      <td>1.256796</td>\n",
       "      <td>0.222228</td>\n",
       "      <td>0.571706</td>\n",
       "      <td>2.897653</td>\n",
       "      <td>0.392902</td>\n",
       "      <td>0.811128</td>\n",
       "      <td>0.892789</td>\n",
       "      <td>0.319290</td>\n",
       "      <td>0.858722</td>\n",
       "      <td>0.591050</td>\n",
       "      <td>0.579851</td>\n",
       "      <td>0.726817</td>\n",
       "      <td>0.748457</td>\n",
       "      <td>0.124196</td>\n",
       "      <td>0.366415</td>\n",
       "      <td>0.300446</td>\n",
       "      <td>0.698067</td>\n",
       "      <td>0.078461</td>\n",
       "      <td>0.187983</td>\n",
       "      <td>0.496719</td>\n",
       "      <td>0.070752</td>\n",
       "      <td>0.532306</td>\n",
       "      <td>1.128576</td>\n",
       "      <td>0.393549</td>\n",
       "      <td>0.874915</td>\n",
       "      <td>0.457772</td>\n",
       "      <td>0.812421</td>\n",
       "      <td>0.264941</td>\n",
       "      <td>0.380119</td>\n",
       "      <td>0.126135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17862.784315</td>\n",
       "      <td>1.52533</td>\n",
       "      <td>1.252073</td>\n",
       "      <td>2.934818</td>\n",
       "      <td>2.788005</td>\n",
       "      <td>0.438902</td>\n",
       "      <td>0.215333</td>\n",
       "      <td>1.030102</td>\n",
       "      <td>2.255770</td>\n",
       "      <td>3.474822</td>\n",
       "      <td>1.083340</td>\n",
       "      <td>3.042333</td>\n",
       "      <td>0.567089</td>\n",
       "      <td>2.014697</td>\n",
       "      <td>3.163212</td>\n",
       "      <td>3.862236</td>\n",
       "      <td>2.226163</td>\n",
       "      <td>1.477436</td>\n",
       "      <td>1.335985</td>\n",
       "      <td>4.636145</td>\n",
       "      <td>1.438727</td>\n",
       "      <td>0.696050</td>\n",
       "      <td>1.446220</td>\n",
       "      <td>0.782979</td>\n",
       "      <td>4.629015</td>\n",
       "      <td>2.332994</td>\n",
       "      <td>1.710305</td>\n",
       "      <td>2.873222</td>\n",
       "      <td>0.828112</td>\n",
       "      <td>1.901294</td>\n",
       "      <td>1.640880</td>\n",
       "      <td>0.897354</td>\n",
       "      <td>2.416849</td>\n",
       "      <td>1.310202</td>\n",
       "      <td>3.368622</td>\n",
       "      <td>3.197965</td>\n",
       "      <td>2.555119</td>\n",
       "      <td>0.756934</td>\n",
       "      <td>1.602579</td>\n",
       "      <td>3.298315</td>\n",
       "      <td>3.299798</td>\n",
       "      <td>1.085672</td>\n",
       "      <td>1.961189</td>\n",
       "      <td>1.706013</td>\n",
       "      <td>1.346090</td>\n",
       "      <td>2.587131</td>\n",
       "      <td>2.348359</td>\n",
       "      <td>1.446203</td>\n",
       "      <td>2.577071</td>\n",
       "      <td>1.369380</td>\n",
       "      <td>1.720470</td>\n",
       "      <td>0.513820</td>\n",
       "      <td>1.044788</td>\n",
       "      <td>2.006485</td>\n",
       "      <td>4.113319</td>\n",
       "      <td>0.998743</td>\n",
       "      <td>1.925806</td>\n",
       "      <td>1.082148</td>\n",
       "      <td>3.983722</td>\n",
       "      <td>2.577693</td>\n",
       "      <td>3.028469</td>\n",
       "      <td>1.017553</td>\n",
       "      <td>2.666742</td>\n",
       "      <td>0.946158</td>\n",
       "      <td>3.402080</td>\n",
       "      <td>0.783052</td>\n",
       "      <td>1.361874</td>\n",
       "      <td>4.974322</td>\n",
       "      <td>1.761054</td>\n",
       "      <td>4.111091</td>\n",
       "      <td>1.941368</td>\n",
       "      <td>1.162443</td>\n",
       "      <td>2.411646</td>\n",
       "      <td>5.783233</td>\n",
       "      <td>3.757822</td>\n",
       "      <td>3.200095</td>\n",
       "      <td>2.920038</td>\n",
       "      <td>0.906621</td>\n",
       "      <td>2.778317</td>\n",
       "      <td>1.285569</td>\n",
       "      <td>2.245671</td>\n",
       "      <td>0.461244</td>\n",
       "      <td>0.836269</td>\n",
       "      <td>2.434921</td>\n",
       "      <td>1.151460</td>\n",
       "      <td>1.900438</td>\n",
       "      <td>2.681554</td>\n",
       "      <td>1.575455</td>\n",
       "      <td>2.115466</td>\n",
       "      <td>1.527385</td>\n",
       "      <td>4.597804</td>\n",
       "      <td>2.045646</td>\n",
       "      <td>0.982385</td>\n",
       "      <td>1.201720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15470.250000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30939.500000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>46408.750000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61.00000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>87.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id       feat_1        feat_2        feat_3        feat_4  \\\n",
       "count  61878.000000  61878.00000  61878.000000  61878.000000  61878.000000   \n",
       "mean   30939.500000      0.38668      0.263066      0.901467      0.779081   \n",
       "std    17862.784315      1.52533      1.252073      2.934818      2.788005   \n",
       "min        1.000000      0.00000      0.000000      0.000000      0.000000   \n",
       "25%    15470.250000      0.00000      0.000000      0.000000      0.000000   \n",
       "50%    30939.500000      0.00000      0.000000      0.000000      0.000000   \n",
       "75%    46408.750000      0.00000      0.000000      0.000000      0.000000   \n",
       "max    61878.000000     61.00000     51.000000     64.000000     70.000000   \n",
       "\n",
       "             feat_5        feat_6        feat_7        feat_8        feat_9  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.071043      0.025696      0.193704      0.662433      1.011296   \n",
       "std        0.438902      0.215333      1.030102      2.255770      3.474822   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "max       19.000000     10.000000     38.000000     76.000000     43.000000   \n",
       "\n",
       "            feat_10       feat_11       feat_12       feat_13       feat_14  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.263906      1.252869      0.140874      0.480979      1.696693   \n",
       "std        1.083340      3.042333      0.567089      2.014697      3.163212   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      1.000000      0.000000      0.000000      2.000000   \n",
       "max       30.000000     38.000000     30.000000     72.000000     33.000000   \n",
       "\n",
       "            feat_15       feat_16       feat_17       feat_18       feat_19  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       1.284398      1.413459      0.366108      0.575423      0.551699   \n",
       "std        3.862236      2.226163      1.477436      1.335985      4.636145   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      2.000000      0.000000      1.000000      0.000000   \n",
       "max       46.000000     37.000000     43.000000     32.000000    121.000000   \n",
       "\n",
       "            feat_20       feat_21       feat_22       feat_23       feat_24  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.471525      0.204014      0.729969      0.142522      2.643880   \n",
       "std        1.438727      0.696050      1.446220      0.782979      4.629015   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      1.000000   \n",
       "75%        0.000000      0.000000      1.000000      0.000000      3.000000   \n",
       "max       27.000000     14.000000     22.000000     64.000000    263.000000   \n",
       "\n",
       "            feat_25       feat_26       feat_27       feat_28       feat_29  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       1.534520      0.563108      0.696613      0.238970      0.275768   \n",
       "std        2.332994      1.710305      2.873222      0.828112      1.901294   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        2.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max       30.000000     33.000000    123.000000     22.000000     69.000000   \n",
       "\n",
       "            feat_30       feat_31       feat_32       feat_33       feat_34  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.150312      0.148680      1.043796      0.696516      0.946411   \n",
       "std        1.640880      0.897354      2.416849      1.310202      3.368622   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      1.000000      1.000000      1.000000   \n",
       "max       87.000000     59.000000    149.000000     24.000000     84.000000   \n",
       "\n",
       "            feat_35       feat_36       feat_37       feat_38       feat_39  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.666263      0.709089      0.263632      0.582129      0.485585   \n",
       "std        3.197965      2.555119      0.756934      1.602579      3.298315   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      1.000000      0.000000      1.000000      0.000000   \n",
       "max      105.000000     84.000000     22.000000     39.000000     78.000000   \n",
       "\n",
       "            feat_40       feat_41       feat_42       feat_43       feat_44  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       1.653059      0.303468      0.698019      0.451146      0.560829   \n",
       "std        3.299798      1.085672      1.961189      1.706013      1.346090   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        2.000000      0.000000      1.000000      0.000000      1.000000   \n",
       "max       41.000000     36.000000     41.000000     42.000000     34.000000   \n",
       "\n",
       "            feat_45       feat_46       feat_47       feat_48       feat_49  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.238130      0.641375      0.249669      1.584893      0.348314   \n",
       "std        2.587131      2.348359      1.446203      2.577071      1.369380   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      2.000000      0.000000   \n",
       "max       80.000000     41.000000     47.000000     49.000000     81.000000   \n",
       "\n",
       "            feat_50       feat_51       feat_52       feat_53       feat_54  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.324283      0.053298      0.213485      0.442063      2.072465   \n",
       "std        1.720470      0.513820      1.044788      2.006485      4.113319   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      2.000000   \n",
       "max       73.000000     44.000000     48.000000     53.000000     63.000000   \n",
       "\n",
       "            feat_55       feat_56       feat_57       feat_58       feat_59  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.323120      0.303775      0.309108      0.697970      0.388603   \n",
       "std        0.998743      1.925806      1.082148      3.983722      2.577693   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max       27.000000     62.000000     30.000000    117.000000     97.000000   \n",
       "\n",
       "            feat_60       feat_61       feat_62       feat_63       feat_64  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       1.029930      0.239746      1.187563      0.168590      1.256796   \n",
       "std        3.028469      1.017553      2.666742      0.946158      3.402080   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      1.000000      0.000000      1.000000   \n",
       "max       40.000000     38.000000     56.000000     51.000000     73.000000   \n",
       "\n",
       "            feat_65       feat_66       feat_67       feat_68       feat_69  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.222228      0.571706      2.897653      0.392902      0.811128   \n",
       "std        0.783052      1.361874      4.974322      1.761054      4.111091   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      1.000000      0.000000      0.000000   \n",
       "75%        0.000000      1.000000      4.000000      0.000000      0.000000   \n",
       "max       38.000000     36.000000    104.000000    109.000000     76.000000   \n",
       "\n",
       "            feat_70       feat_71       feat_72       feat_73       feat_74  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.892789      0.319290      0.858722      0.591050      0.579851   \n",
       "std        1.941368      1.162443      2.411646      5.783233      3.757822   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      0.000000      1.000000      0.000000      0.000000   \n",
       "max       46.000000     31.000000     30.000000    352.000000    231.000000   \n",
       "\n",
       "            feat_75       feat_76       feat_77       feat_78       feat_79  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.726817      0.748457      0.124196      0.366415      0.300446   \n",
       "std        3.200095      2.920038      0.906621      2.778317      1.285569   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max       80.000000    102.000000     29.000000     80.000000     25.000000   \n",
       "\n",
       "            feat_80       feat_81       feat_82       feat_83       feat_84  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.698067      0.078461      0.187983      0.496719      0.070752   \n",
       "std        2.245671      0.461244      0.836269      2.434921      1.151460   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max       54.000000     26.000000     24.000000     79.000000     76.000000   \n",
       "\n",
       "            feat_85       feat_86       feat_87       feat_88       feat_89  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.532306      1.128576      0.393549      0.874915      0.457772   \n",
       "std        1.900438      2.681554      1.575455      2.115466      1.527385   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      1.000000      0.000000      1.000000      0.000000   \n",
       "max       55.000000     65.000000     67.000000     30.000000     61.000000   \n",
       "\n",
       "            feat_90       feat_91       feat_92       feat_93  \n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  \n",
       "mean       0.812421      0.264941      0.380119      0.126135  \n",
       "std        4.597804      2.045646      0.982385      1.201720  \n",
       "min        0.000000      0.000000      0.000000      0.000000  \n",
       "25%        0.000000      0.000000      0.000000      0.000000  \n",
       "50%        0.000000      0.000000      0.000000      0.000000  \n",
       "75%        0.000000      0.000000      0.000000      0.000000  \n",
       "max      130.000000     52.000000     19.000000     87.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_all(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63c3a5e4-bea2-4483-a180-712bb7f7416e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, objective=&#x27;multi:softprob&#x27;,\n",
       "              predictor=&#x27;auto&#x27;, random_state=0, reg_alpha=0, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, objective=&#x27;multi:softprob&#x27;,\n",
       "              predictor=&#x27;auto&#x27;, random_state=0, reg_alpha=0, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, objective='multi:softprob',\n",
       "              predictor='auto', random_state=0, reg_alpha=0, ...)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(use_label_encoder=False)\n",
    "xgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10810861-0299-4651-80cf-4d74ada0eeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data logloss: 0.48188992358736077\n"
     ]
    }
   ],
   "source": [
    "pred = xgb.predict_proba(x_val)\n",
    "print(\"Validation data logloss: {}\".format(log_loss(y_val, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "512c247c-3302-4b5e-b3b3-64b4d2f1d8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.42%\n"
     ]
    }
   ],
   "source": [
    "pred = xgb.predict(x_val)\n",
    "accu = accuracy_score(y_val, pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accu * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fb3e01e-e8ba-4e31-a935-571015b90a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate=0.345, logloss: 0.47723\n",
      "learning_rate=0.350, logloss: 0.47421\n",
      "learning_rate=0.355, logloss: 0.47629\n"
     ]
    }
   ],
   "source": [
    "lr_scores = []\n",
    "learning_rates = [0.345, 0.350, 0.355]\n",
    "\n",
    "for l in learning_rates:\n",
    "    tune_xgb = XGBClassifier(use_label_encoder=False, learning_rate=l)\n",
    "    tune_xgb.fit(x_train, y_train)\n",
    "    pred = tune_xgb.predict_proba(x_val)\n",
    "    lr_scores.append(log_loss(y_val, pred))\n",
    "    print(\"learning_rate=%.3f, logloss: %.5f\" % (l, log_loss(y_val, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7295515-0464-40f4-bf89-c028fee5c6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning_rate 0.35\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(learning_rates, lr_scores, 'o-')\n",
    "plt.ylabel(log_loss)\n",
    "plt.xlabel(\"learning_rate\")\n",
    "print(\"best learning_rate {}\".format(learning_rates[np.argmin(lr_scores)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92b6fcd8-6df4-4b66-a273-a69d539a604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = {'objective':'multi:sofprob',\n",
    "      'learning_rate': 0.35,\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2796a320-5860-422e-958c-04346a46fd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CalibratedClassifierCV(base_estimator=XGBClassifier(base_score=None,\n",
       "                                                    booster=None,\n",
       "                                                    callbacks=None,\n",
       "                                                    colsample_bylevel=None,\n",
       "                                                    colsample_bynode=None,\n",
       "                                                    colsample_bytree=None,\n",
       "                                                    early_stopping_rounds=None,\n",
       "                                                    enable_categorical=False,\n",
       "                                                    eval_metric=None,\n",
       "                                                    gamma=None, gpu_id=None,\n",
       "                                                    grow_policy=None,\n",
       "                                                    importance_type=None,\n",
       "                                                    interaction_constraints=None,\n",
       "                                                    learning_rate=0.35,\n",
       "                                                    max_bin=None,\n",
       "                                                    max_cat_to_onehot=None,\n",
       "                                                    max_delta_step=None,\n",
       "                                                    max_depth=None,\n",
       "                                                    max_leaves=None,\n",
       "                                                    min_child_weight=None,\n",
       "                                                    missing=nan,\n",
       "                                                    monotone_constraints=None,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs=None,\n",
       "                                                    num_parallel_tree=None,\n",
       "                                                    objective=&#x27;multi:sofprob&#x27;,\n",
       "                                                    predictor=None,\n",
       "                                                    random_state=None,\n",
       "                                                    reg_alpha=None, ...),\n",
       "                       cv=5, method=&#x27;isotonic&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CalibratedClassifierCV</label><div class=\"sk-toggleable__content\"><pre>CalibratedClassifierCV(base_estimator=XGBClassifier(base_score=None,\n",
       "                                                    booster=None,\n",
       "                                                    callbacks=None,\n",
       "                                                    colsample_bylevel=None,\n",
       "                                                    colsample_bynode=None,\n",
       "                                                    colsample_bytree=None,\n",
       "                                                    early_stopping_rounds=None,\n",
       "                                                    enable_categorical=False,\n",
       "                                                    eval_metric=None,\n",
       "                                                    gamma=None, gpu_id=None,\n",
       "                                                    grow_policy=None,\n",
       "                                                    importance_type=None,\n",
       "                                                    interaction_constraints=None,\n",
       "                                                    learning_rate=0.35,\n",
       "                                                    max_bin=None,\n",
       "                                                    max_cat_to_onehot=None,\n",
       "                                                    max_delta_step=None,\n",
       "                                                    max_depth=None,\n",
       "                                                    max_leaves=None,\n",
       "                                                    min_child_weight=None,\n",
       "                                                    missing=nan,\n",
       "                                                    monotone_constraints=None,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs=None,\n",
       "                                                    num_parallel_tree=None,\n",
       "                                                    objective=&#x27;multi:sofprob&#x27;,\n",
       "                                                    predictor=None,\n",
       "                                                    random_state=None,\n",
       "                                                    reg_alpha=None, ...),\n",
       "                       cv=5, method=&#x27;isotonic&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">base_estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, gamma=None,\n",
       "              gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.35, max_bin=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:sofprob&#x27;, predictor=None,\n",
       "              random_state=None, reg_alpha=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, gamma=None,\n",
       "              gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.35, max_bin=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:sofprob&#x27;, predictor=None,\n",
       "              random_state=None, reg_alpha=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=XGBClassifier(base_score=None,\n",
       "                                                    booster=None,\n",
       "                                                    callbacks=None,\n",
       "                                                    colsample_bylevel=None,\n",
       "                                                    colsample_bynode=None,\n",
       "                                                    colsample_bytree=None,\n",
       "                                                    early_stopping_rounds=None,\n",
       "                                                    enable_categorical=False,\n",
       "                                                    eval_metric=None,\n",
       "                                                    gamma=None, gpu_id=None,\n",
       "                                                    grow_policy=None,\n",
       "                                                    importance_type=None,\n",
       "                                                    interaction_constraints=None,\n",
       "                                                    learning_rate=0.35,\n",
       "                                                    max_bin=None,\n",
       "                                                    max_cat_to_onehot=None,\n",
       "                                                    max_delta_step=None,\n",
       "                                                    max_depth=None,\n",
       "                                                    max_leaves=None,\n",
       "                                                    min_child_weight=None,\n",
       "                                                    missing=nan,\n",
       "                                                    monotone_constraints=None,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs=None,\n",
       "                                                    num_parallel_tree=None,\n",
       "                                                    objective='multi:sofprob',\n",
       "                                                    predictor=None,\n",
       "                                                    random_state=None,\n",
       "                                                    reg_alpha=None, ...),\n",
       "                       cv=5, method='isotonic')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(**pa, use_label_encoder=False)\n",
    "model = CalibratedClassifierCV(xgb, cv=5, method=\"isotonic\")\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50c60fc4-f2d9-454f-b8a7-3b3352da516a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data logloss: 0.4628408700108092\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict_proba(x_val)\n",
    "print(\"Validation data logloss: {}\".format(log_loss(y_val, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d82bb539-46d8-4100-8b45-c9df9ed7df6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     1,      0,      0, ...,      0,      0,      0],\n",
       "       [     2,      2,      2, ...,      0,      2,      0],\n",
       "       [     3,      0,      1, ...,      0,      0,      1],\n",
       "       ...,\n",
       "       [144366,      0,      1, ...,      1,      0,      0],\n",
       "       [144367,      0,      0, ...,      0,      1,      0],\n",
       "       [144368,      0,      0, ...,      0,      0,      0]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = test.values\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd754b28-06ae-4016-9348-8474dc076df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144368, 94)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ad2f9e0-bfbb-4d55-ae63-c33568da73f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 2,  2, 14, ...,  0,  2,  0],\n",
       "       [ 0,  1, 12, ...,  0,  0,  1],\n",
       "       ...,\n",
       "       [ 0,  1,  0, ...,  1,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  1,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = ds[:, 1:94]\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7d8dad6-4bb0-4121-988d-79e7580b59db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 1.88199355e-01, 1.06343002e-01, ...,\n",
       "        6.91058381e-03, 1.08569352e-04, 0.00000000e+00],\n",
       "       [3.16431650e-03, 8.38208680e-03, 5.35171004e-04, ...,\n",
       "        3.92670367e-03, 5.24648612e-01, 8.28392614e-04],\n",
       "       [0.00000000e+00, 7.04736966e-05, 0.00000000e+00, ...,\n",
       "        1.57883358e-04, 6.02903299e-04, 1.59141399e-04],\n",
       "       ...,\n",
       "       [0.00000000e+00, 5.55323526e-01, 2.49213800e-01, ...,\n",
       "        6.59905955e-03, 1.01935449e-04, 0.00000000e+00],\n",
       "       [0.00000000e+00, 1.99353055e-01, 8.03249086e-03, ...,\n",
       "        2.21419337e-04, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 5.56113289e-01, 3.01602483e-01, ...,\n",
       "        8.48387658e-02, 1.01244940e-04, 4.72638293e-05]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict_proba(x_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3b2ae1e-b013-494d-bc23-c423296145bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class_1</th>\n",
       "      <th>Class_2</th>\n",
       "      <th>Class_3</th>\n",
       "      <th>Class_4</th>\n",
       "      <th>Class_5</th>\n",
       "      <th>Class_6</th>\n",
       "      <th>Class_7</th>\n",
       "      <th>Class_8</th>\n",
       "      <th>Class_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188199</td>\n",
       "      <td>0.106343</td>\n",
       "      <td>0.698009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.006911</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003164</td>\n",
       "      <td>0.008382</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.457962</td>\n",
       "      <td>0.003927</td>\n",
       "      <td>0.524649</td>\n",
       "      <td>0.000828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999010</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.611847</td>\n",
       "      <td>0.365047</td>\n",
       "      <td>0.021719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.001118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.131924</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005973</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>0.089794</td>\n",
       "      <td>0.768140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144364</th>\n",
       "      <td>0.653032</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.264888</td>\n",
       "      <td>0.023379</td>\n",
       "      <td>0.013318</td>\n",
       "      <td>0.035842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144365</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311804</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>0.063729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.097270</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144366</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555324</td>\n",
       "      <td>0.249214</td>\n",
       "      <td>0.188328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.006599</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144367</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199353</td>\n",
       "      <td>0.008032</td>\n",
       "      <td>0.792393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144368</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.556113</td>\n",
       "      <td>0.301602</td>\n",
       "      <td>0.055888</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>0.084839</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144368 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Class_1   Class_2   Class_3   Class_4   Class_5   Class_6   Class_7  \\\n",
       "id                                                                             \n",
       "1       0.000000  0.188199  0.106343  0.698009  0.000000  0.000429  0.006911   \n",
       "2       0.003164  0.008382  0.000535  0.000337  0.000216  0.457962  0.003927   \n",
       "3       0.000000  0.000070  0.000000  0.000000  0.000000  0.999010  0.000158   \n",
       "4       0.000000  0.611847  0.365047  0.021719  0.000000  0.000000  0.000161   \n",
       "5       0.131924  0.000069  0.001029  0.000182  0.000000  0.005973  0.002889   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "144364  0.653032  0.002491  0.004095  0.002956  0.000000  0.264888  0.023379   \n",
       "144365  0.000000  0.311804  0.525800  0.063729  0.000000  0.001084  0.097270   \n",
       "144366  0.000000  0.555324  0.249214  0.188328  0.000000  0.000433  0.006599   \n",
       "144367  0.000000  0.199353  0.008032  0.792393  0.000000  0.000000  0.000221   \n",
       "144368  0.000000  0.556113  0.301602  0.055888  0.000204  0.001205  0.084839   \n",
       "\n",
       "         Class_8   Class_9  \n",
       "id                          \n",
       "1       0.000109  0.000000  \n",
       "2       0.524649  0.000828  \n",
       "3       0.000603  0.000159  \n",
       "4       0.000108  0.001118  \n",
       "5       0.089794  0.768140  \n",
       "...          ...       ...  \n",
       "144364  0.013318  0.035842  \n",
       "144365  0.000100  0.000213  \n",
       "144366  0.000102  0.000000  \n",
       "144367  0.000000  0.000000  \n",
       "144368  0.000101  0.000047  \n",
       "\n",
       "[144368 rows x 9 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame(pred, columns=[\"Class_1\",\"Class_2\",\"Class_3\",\"Class_4\",\"Class_5\",\"Class_6\",\"Class_7\",\"Class_8\",\"Class_9\"])\n",
    "output.index.name = \"id\"\n",
    "output.index += 1\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6e3553-2ad7-499d-82c9-b765dae449c4",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7757a5ca-1fa8-42ed-a075-99fc53e2e640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61873</th>\n",
       "      <td>61874</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61874</th>\n",
       "      <td>61875</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61875</th>\n",
       "      <td>61876</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61876</th>\n",
       "      <td>61877</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61877</th>\n",
       "      <td>61878</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61878 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "0          1       1       0       0       0       0       0       0       0   \n",
       "1          2       0       0       0       0       0       0       0       1   \n",
       "2          3       0       0       0       0       0       0       0       1   \n",
       "3          4       1       0       0       1       6       1       5       0   \n",
       "4          5       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "61873  61874       1       0       0       1       1       0       0       0   \n",
       "61874  61875       4       0       0       0       0       0       0       0   \n",
       "61875  61876       0       0       0       0       0       0       0       3   \n",
       "61876  61877       1       0       0       0       0       0       0       0   \n",
       "61877  61878       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       feat_9  ...  feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  \\\n",
       "0           0  ...        1        0        0        0        0        0   \n",
       "1           0  ...        0        0        0        0        0        0   \n",
       "2           0  ...        0        0        0        0        0        0   \n",
       "3           0  ...        0        1        2        0        0        0   \n",
       "4           0  ...        1        0        0        0        0        1   \n",
       "...       ...  ...      ...      ...      ...      ...      ...      ...   \n",
       "61873       0  ...        1        0        0        0        0        0   \n",
       "61874       0  ...        0        2        0        0        2        0   \n",
       "61875       1  ...        0        3        1        0        0        0   \n",
       "61876       0  ...        0        0        0        0        1        0   \n",
       "61877       0  ...        0        0        0        0        0        0   \n",
       "\n",
       "       feat_91  feat_92  feat_93   target  \n",
       "0            0        0        0  Class_1  \n",
       "1            0        0        0  Class_1  \n",
       "2            0        0        0  Class_1  \n",
       "3            0        0        0  Class_1  \n",
       "4            0        0        0  Class_1  \n",
       "...        ...      ...      ...      ...  \n",
       "61873        0        2        0  Class_9  \n",
       "61874        0        1        0  Class_9  \n",
       "61875        0        0        0  Class_9  \n",
       "61876        3       10        0  Class_9  \n",
       "61877        0        2        0  Class_9  \n",
       "\n",
       "[61878 rows x 95 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data.copy()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ca9f553-cf99-46d2-9420-8cc65309fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"target\"] = train[\"target\"].str.replace('Class_', '')\n",
    "train[\"target\"] = train[\"target\"].astype(int) - 1\n",
    "\n",
    "X_train = train.drop(['id','target'] , axis=1)\n",
    "y_train = train[\"target\"]\n",
    "X_test = test.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32311297-1d2f-4cfa-8867-3d97c8cfd1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.zeros((len(X_test), 9))\n",
    "models = []\n",
    "oof_train = np.zeros((len(X_train),9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "213165c2-14b8-4b8e-b4a5-55010bf7b2ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\09de1\\Documents\\python_basic\\venv_python_basic\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\09de1\\Documents\\python_basic\\venv_python_basic\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028951 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3123\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score -3.457968\n",
      "[LightGBM] [Info] Start training from score -1.343469\n",
      "[LightGBM] [Info] Start training from score -2.036693\n",
      "[LightGBM] [Info] Start training from score -3.148241\n",
      "[LightGBM] [Info] Start training from score -3.114920\n",
      "[LightGBM] [Info] Start training from score -1.476149\n",
      "[LightGBM] [Info] Start training from score -3.083556\n",
      "[LightGBM] [Info] Start training from score -1.990842\n",
      "[LightGBM] [Info] Start training from score -2.539499\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's multi_logloss: 0.823215\tvalid_1's multi_logloss: 0.864415\n",
      "[20]\ttraining's multi_logloss: 0.608748\tvalid_1's multi_logloss: 0.668215\n",
      "[30]\ttraining's multi_logloss: 0.514717\tvalid_1's multi_logloss: 0.592321\n",
      "[40]\ttraining's multi_logloss: 0.458634\tvalid_1's multi_logloss: 0.553393\n",
      "[50]\ttraining's multi_logloss: 0.420538\tvalid_1's multi_logloss: 0.531792\n",
      "[60]\ttraining's multi_logloss: 0.392067\tvalid_1's multi_logloss: 0.518581\n",
      "[70]\ttraining's multi_logloss: 0.369319\tvalid_1's multi_logloss: 0.509848\n",
      "[80]\ttraining's multi_logloss: 0.351004\tvalid_1's multi_logloss: 0.505193\n",
      "[90]\ttraining's multi_logloss: 0.335164\tvalid_1's multi_logloss: 0.500608\n",
      "[100]\ttraining's multi_logloss: 0.32086\tvalid_1's multi_logloss: 0.496796\n",
      "[110]\ttraining's multi_logloss: 0.30717\tvalid_1's multi_logloss: 0.492876\n",
      "[120]\ttraining's multi_logloss: 0.29476\tvalid_1's multi_logloss: 0.490494\n",
      "[130]\ttraining's multi_logloss: 0.2834\tvalid_1's multi_logloss: 0.487789\n",
      "[140]\ttraining's multi_logloss: 0.272622\tvalid_1's multi_logloss: 0.486451\n",
      "[150]\ttraining's multi_logloss: 0.26327\tvalid_1's multi_logloss: 0.486068\n",
      "[160]\ttraining's multi_logloss: 0.254246\tvalid_1's multi_logloss: 0.484941\n",
      "[170]\ttraining's multi_logloss: 0.245245\tvalid_1's multi_logloss: 0.483405\n",
      "[180]\ttraining's multi_logloss: 0.237851\tvalid_1's multi_logloss: 0.482484\n",
      "[190]\ttraining's multi_logloss: 0.230688\tvalid_1's multi_logloss: 0.48217\n",
      "[200]\ttraining's multi_logloss: 0.223631\tvalid_1's multi_logloss: 0.481406\n",
      "Early stopping, best iteration is:\n",
      "[198]\ttraining's multi_logloss: 0.224977\tvalid_1's multi_logloss: 0.481339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\09de1\\Documents\\python_basic\\venv_python_basic\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\09de1\\Documents\\python_basic\\venv_python_basic\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3124\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score -3.479363\n",
      "[LightGBM] [Info] Start training from score -1.344321\n",
      "[LightGBM] [Info] Start training from score -2.046184\n",
      "[LightGBM] [Info] Start training from score -3.143078\n",
      "[LightGBM] [Info] Start training from score -3.113101\n",
      "[LightGBM] [Info] Start training from score -1.476061\n",
      "[LightGBM] [Info] Start training from score -3.101807\n",
      "[LightGBM] [Info] Start training from score -1.981860\n",
      "[LightGBM] [Info] Start training from score -2.521988\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's multi_logloss: 0.824283\tvalid_1's multi_logloss: 0.859057\n",
      "[20]\ttraining's multi_logloss: 0.608323\tvalid_1's multi_logloss: 0.661458\n",
      "[30]\ttraining's multi_logloss: 0.513539\tvalid_1's multi_logloss: 0.586923\n",
      "[40]\ttraining's multi_logloss: 0.457529\tvalid_1's multi_logloss: 0.549512\n",
      "[50]\ttraining's multi_logloss: 0.419869\tvalid_1's multi_logloss: 0.528809\n",
      "[60]\ttraining's multi_logloss: 0.39145\tvalid_1's multi_logloss: 0.515775\n",
      "[70]\ttraining's multi_logloss: 0.368869\tvalid_1's multi_logloss: 0.507557\n",
      "[80]\ttraining's multi_logloss: 0.350846\tvalid_1's multi_logloss: 0.502114\n",
      "[90]\ttraining's multi_logloss: 0.334801\tvalid_1's multi_logloss: 0.49787\n",
      "[100]\ttraining's multi_logloss: 0.319748\tvalid_1's multi_logloss: 0.493267\n",
      "[110]\ttraining's multi_logloss: 0.306695\tvalid_1's multi_logloss: 0.489982\n",
      "[120]\ttraining's multi_logloss: 0.294717\tvalid_1's multi_logloss: 0.487331\n",
      "[130]\ttraining's multi_logloss: 0.28396\tvalid_1's multi_logloss: 0.485929\n",
      "[140]\ttraining's multi_logloss: 0.274239\tvalid_1's multi_logloss: 0.4843\n",
      "[150]\ttraining's multi_logloss: 0.264883\tvalid_1's multi_logloss: 0.48306\n",
      "[160]\ttraining's multi_logloss: 0.255824\tvalid_1's multi_logloss: 0.481487\n",
      "[170]\ttraining's multi_logloss: 0.247256\tvalid_1's multi_logloss: 0.480594\n",
      "[180]\ttraining's multi_logloss: 0.239487\tvalid_1's multi_logloss: 0.48012\n",
      "Early stopping, best iteration is:\n",
      "[178]\ttraining's multi_logloss: 0.240937\tvalid_1's multi_logloss: 0.479802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\09de1\\Documents\\python_basic\\venv_python_basic\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\09de1\\Documents\\python_basic\\venv_python_basic\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015861 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3121\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score -3.461825\n",
      "[LightGBM] [Info] Start training from score -1.347347\n",
      "[LightGBM] [Info] Start training from score -2.053873\n",
      "[LightGBM] [Info] Start training from score -3.124065\n",
      "[LightGBM] [Info] Start training from score -3.119482\n",
      "[LightGBM] [Info] Start training from score -1.477210\n",
      "[LightGBM] [Info] Start training from score -3.073025\n",
      "[LightGBM] [Info] Start training from score -1.989955\n",
      "[LightGBM] [Info] Start training from score -2.512723\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's multi_logloss: 0.827495\tvalid_1's multi_logloss: 0.851461\n",
      "[20]\ttraining's multi_logloss: 0.61117\tvalid_1's multi_logloss: 0.652759\n",
      "[30]\ttraining's multi_logloss: 0.516607\tvalid_1's multi_logloss: 0.576843\n",
      "[40]\ttraining's multi_logloss: 0.460311\tvalid_1's multi_logloss: 0.539033\n",
      "[50]\ttraining's multi_logloss: 0.423131\tvalid_1's multi_logloss: 0.517649\n",
      "[60]\ttraining's multi_logloss: 0.395299\tvalid_1's multi_logloss: 0.505386\n",
      "[70]\ttraining's multi_logloss: 0.372869\tvalid_1's multi_logloss: 0.497831\n",
      "[80]\ttraining's multi_logloss: 0.354124\tvalid_1's multi_logloss: 0.492152\n",
      "[90]\ttraining's multi_logloss: 0.33746\tvalid_1's multi_logloss: 0.487275\n",
      "[100]\ttraining's multi_logloss: 0.32379\tvalid_1's multi_logloss: 0.48429\n",
      "[110]\ttraining's multi_logloss: 0.310151\tvalid_1's multi_logloss: 0.481764\n",
      "[120]\ttraining's multi_logloss: 0.298497\tvalid_1's multi_logloss: 0.479716\n",
      "[130]\ttraining's multi_logloss: 0.287533\tvalid_1's multi_logloss: 0.478206\n",
      "[140]\ttraining's multi_logloss: 0.277092\tvalid_1's multi_logloss: 0.476424\n",
      "[150]\ttraining's multi_logloss: 0.267346\tvalid_1's multi_logloss: 0.474816\n",
      "[160]\ttraining's multi_logloss: 0.258657\tvalid_1's multi_logloss: 0.474151\n",
      "[170]\ttraining's multi_logloss: 0.249904\tvalid_1's multi_logloss: 0.473141\n",
      "[180]\ttraining's multi_logloss: 0.241612\tvalid_1's multi_logloss: 0.472805\n",
      "Early stopping, best iteration is:\n",
      "[179]\ttraining's multi_logloss: 0.242449\tvalid_1's multi_logloss: 0.472779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\09de1\\Documents\\python_basic\\venv_python_basic\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\09de1\\Documents\\python_basic\\venv_python_basic\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3130\n",
      "[LightGBM] [Info] Number of data points in the train set: 49503, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score -3.478728\n",
      "[LightGBM] [Info] Start training from score -1.342174\n",
      "[LightGBM] [Info] Start training from score -2.043862\n",
      "[LightGBM] [Info] Start training from score -3.126845\n",
      "[LightGBM] [Info] Start training from score -3.130075\n",
      "[LightGBM] [Info] Start training from score -1.481576\n",
      "[LightGBM] [Info] Start training from score -3.067821\n",
      "[LightGBM] [Info] Start training from score -1.993232\n",
      "[LightGBM] [Info] Start training from score -2.516739\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's multi_logloss: 0.825059\tvalid_1's multi_logloss: 0.856851\n",
      "[20]\ttraining's multi_logloss: 0.610641\tvalid_1's multi_logloss: 0.662161\n",
      "[30]\ttraining's multi_logloss: 0.515011\tvalid_1's multi_logloss: 0.584517\n",
      "[40]\ttraining's multi_logloss: 0.458989\tvalid_1's multi_logloss: 0.546\n",
      "[50]\ttraining's multi_logloss: 0.420989\tvalid_1's multi_logloss: 0.523652\n",
      "[60]\ttraining's multi_logloss: 0.392444\tvalid_1's multi_logloss: 0.510699\n",
      "[70]\ttraining's multi_logloss: 0.370171\tvalid_1's multi_logloss: 0.502764\n",
      "[80]\ttraining's multi_logloss: 0.351317\tvalid_1's multi_logloss: 0.497862\n",
      "[90]\ttraining's multi_logloss: 0.335017\tvalid_1's multi_logloss: 0.493364\n",
      "[100]\ttraining's multi_logloss: 0.320937\tvalid_1's multi_logloss: 0.490777\n",
      "[110]\ttraining's multi_logloss: 0.307875\tvalid_1's multi_logloss: 0.488366\n",
      "[120]\ttraining's multi_logloss: 0.296424\tvalid_1's multi_logloss: 0.487568\n",
      "[130]\ttraining's multi_logloss: 0.285353\tvalid_1's multi_logloss: 0.485773\n",
      "[140]\ttraining's multi_logloss: 0.274609\tvalid_1's multi_logloss: 0.484355\n",
      "[150]\ttraining's multi_logloss: 0.264247\tvalid_1's multi_logloss: 0.483105\n",
      "[160]\ttraining's multi_logloss: 0.255202\tvalid_1's multi_logloss: 0.482159\n",
      "[170]\ttraining's multi_logloss: 0.246579\tvalid_1's multi_logloss: 0.481245\n",
      "[180]\ttraining's multi_logloss: 0.238553\tvalid_1's multi_logloss: 0.480721\n",
      "[190]\ttraining's multi_logloss: 0.231261\tvalid_1's multi_logloss: 0.479881\n",
      "[200]\ttraining's multi_logloss: 0.22384\tvalid_1's multi_logloss: 0.479359\n",
      "[210]\ttraining's multi_logloss: 0.217208\tvalid_1's multi_logloss: 0.479243\n",
      "[220]\ttraining's multi_logloss: 0.211089\tvalid_1's multi_logloss: 0.479079\n",
      "[230]\ttraining's multi_logloss: 0.205661\tvalid_1's multi_logloss: 0.480675\n",
      "Early stopping, best iteration is:\n",
      "[222]\ttraining's multi_logloss: 0.209674\tvalid_1's multi_logloss: 0.478538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\09de1\\Documents\\python_basic\\venv_python_basic\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\09de1\\Documents\\python_basic\\venv_python_basic\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3111\n",
      "[LightGBM] [Info] Number of data points in the train set: 49503, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score -3.463133\n",
      "[LightGBM] [Info] Start training from score -1.347601\n",
      "[LightGBM] [Info] Start training from score -2.045579\n",
      "[LightGBM] [Info] Start training from score -3.134243\n",
      "[LightGBM] [Info] Start training from score -3.110399\n",
      "[LightGBM] [Info] Start training from score -1.471583\n",
      "[LightGBM] [Info] Start training from score -3.082694\n",
      "[LightGBM] [Info] Start training from score -1.990862\n",
      "[LightGBM] [Info] Start training from score -2.533139\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's multi_logloss: 0.825677\tvalid_1's multi_logloss: 0.860416\n",
      "[20]\ttraining's multi_logloss: 0.60936\tvalid_1's multi_logloss: 0.6631\n",
      "[30]\ttraining's multi_logloss: 0.514501\tvalid_1's multi_logloss: 0.585142\n",
      "[40]\ttraining's multi_logloss: 0.458442\tvalid_1's multi_logloss: 0.547347\n",
      "[50]\ttraining's multi_logloss: 0.420085\tvalid_1's multi_logloss: 0.525277\n",
      "[60]\ttraining's multi_logloss: 0.391934\tvalid_1's multi_logloss: 0.511785\n",
      "[70]\ttraining's multi_logloss: 0.369883\tvalid_1's multi_logloss: 0.502964\n",
      "[80]\ttraining's multi_logloss: 0.351139\tvalid_1's multi_logloss: 0.496942\n",
      "[90]\ttraining's multi_logloss: 0.335175\tvalid_1's multi_logloss: 0.492757\n",
      "[100]\ttraining's multi_logloss: 0.320919\tvalid_1's multi_logloss: 0.488898\n",
      "[110]\ttraining's multi_logloss: 0.307849\tvalid_1's multi_logloss: 0.485345\n",
      "[120]\ttraining's multi_logloss: 0.295884\tvalid_1's multi_logloss: 0.483284\n",
      "[130]\ttraining's multi_logloss: 0.284662\tvalid_1's multi_logloss: 0.481099\n",
      "[140]\ttraining's multi_logloss: 0.273955\tvalid_1's multi_logloss: 0.479697\n",
      "[150]\ttraining's multi_logloss: 0.26399\tvalid_1's multi_logloss: 0.478572\n",
      "[160]\ttraining's multi_logloss: 0.25499\tvalid_1's multi_logloss: 0.477395\n",
      "[170]\ttraining's multi_logloss: 0.246405\tvalid_1's multi_logloss: 0.47579\n",
      "[180]\ttraining's multi_logloss: 0.238552\tvalid_1's multi_logloss: 0.474659\n",
      "[190]\ttraining's multi_logloss: 0.231414\tvalid_1's multi_logloss: 0.474191\n",
      "[200]\ttraining's multi_logloss: 0.224391\tvalid_1's multi_logloss: 0.473324\n",
      "[210]\ttraining's multi_logloss: 0.217818\tvalid_1's multi_logloss: 0.472991\n",
      "[220]\ttraining's multi_logloss: 0.211548\tvalid_1's multi_logloss: 0.472874\n",
      "Early stopping, best iteration is:\n",
      "[218]\ttraining's multi_logloss: 0.212707\tvalid_1's multi_logloss: 0.472699\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "NFOLDS = 5\n",
    "\n",
    "cv = KFold(n_splits=NFOLDS, shuffle=True, random_state=0)\n",
    "\n",
    "params = {\n",
    "    #'metric':'multi_logloss',\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 9,\n",
    "    #'verbosity': 1,\n",
    "}\n",
    "\n",
    "for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train)):\n",
    "    X_tr = X_train.loc[train_index, :]\n",
    "    X_val = X_train.loc[valid_index, :]\n",
    "    y_tr = y_train[train_index].astype(int)\n",
    "    y_val = y_train[valid_index].astype(int)\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_tr, y_tr)\n",
    "    lgb_eval = lgb.Dataset(X_val, y_val)\n",
    "    \n",
    "    model = lgb.train(params, lgb_train,\n",
    "                        valid_sets=[lgb_train, lgb_eval],\n",
    "                        verbose_eval=10,\n",
    "                        num_boost_round=1000,\n",
    "                        early_stopping_rounds=10)\n",
    "    \n",
    "    oof_train[valid_index] = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    y_pred += model.predict(X_test, num_iteration=model.best_iteration)/NFOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d8f8eb9-51c3-47f1-ac5f-93b31043f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSubmission = pd.read_csv('sampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2d0f48c-23a9-4674-b443-363a1ebde0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.concat([sampleSubmission[['id']], pd.DataFrame(y_pred)], axis = 1)\n",
    "submit.columns = sampleSubmission.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c327e3d-178a-4e1c-9148-ba3fb7a0d5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Class_1</th>\n",
       "      <th>Class_2</th>\n",
       "      <th>Class_3</th>\n",
       "      <th>Class_4</th>\n",
       "      <th>Class_5</th>\n",
       "      <th>Class_6</th>\n",
       "      <th>Class_7</th>\n",
       "      <th>Class_8</th>\n",
       "      <th>Class_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.122305</td>\n",
       "      <td>0.179536</td>\n",
       "      <td>0.693218</td>\n",
       "      <td>1.187347e-07</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.004264</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.014184</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>7.970077e-06</td>\n",
       "      <td>0.717289</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.264772</td>\n",
       "      <td>0.000655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>6.824787e-09</td>\n",
       "      <td>0.999524</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.603163</td>\n",
       "      <td>0.391107</td>\n",
       "      <td>0.004824</td>\n",
       "      <td>7.746995e-08</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.171684</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>1.209310e-06</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>0.009849</td>\n",
       "      <td>0.114888</td>\n",
       "      <td>0.698732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144363</th>\n",
       "      <td>144364</td>\n",
       "      <td>0.390072</td>\n",
       "      <td>0.009499</td>\n",
       "      <td>0.007277</td>\n",
       "      <td>0.007681</td>\n",
       "      <td>8.615828e-07</td>\n",
       "      <td>0.490272</td>\n",
       "      <td>0.033270</td>\n",
       "      <td>0.019173</td>\n",
       "      <td>0.042754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144364</th>\n",
       "      <td>144365</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.305566</td>\n",
       "      <td>0.547436</td>\n",
       "      <td>0.079166</td>\n",
       "      <td>1.369958e-06</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.066321</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144365</th>\n",
       "      <td>144366</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.653945</td>\n",
       "      <td>0.237071</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>2.511128e-07</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144366</th>\n",
       "      <td>144367</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.155263</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.831956</td>\n",
       "      <td>3.728673e-08</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144367</th>\n",
       "      <td>144368</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.533171</td>\n",
       "      <td>0.383180</td>\n",
       "      <td>0.035643</td>\n",
       "      <td>1.460643e-06</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.046507</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144368 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id   Class_1   Class_2   Class_3   Class_4       Class_5  \\\n",
       "0            1  0.000282  0.122305  0.179536  0.693218  1.187347e-07   \n",
       "1            2  0.000812  0.014184  0.000825  0.000370  7.970077e-06   \n",
       "2            3  0.000005  0.000027  0.000017  0.000004  6.824787e-09   \n",
       "3            4  0.000052  0.603163  0.391107  0.004824  7.746995e-08   \n",
       "4            5  0.171684  0.000384  0.000599  0.000188  1.209310e-06   \n",
       "...        ...       ...       ...       ...       ...           ...   \n",
       "144363  144364  0.390072  0.009499  0.007277  0.007681  8.615828e-07   \n",
       "144364  144365  0.000400  0.305566  0.547436  0.079166  1.369958e-06   \n",
       "144365  144366  0.000148  0.653945  0.237071  0.107045  2.511128e-07   \n",
       "144366  144367  0.000025  0.155263  0.012583  0.831956  3.728673e-08   \n",
       "144367  144368  0.000630  0.533171  0.383180  0.035643  1.460643e-06   \n",
       "\n",
       "         Class_6   Class_7   Class_8   Class_9  \n",
       "0       0.000217  0.004264  0.000167  0.000013  \n",
       "1       0.717289  0.001086  0.264772  0.000655  \n",
       "2       0.999524  0.000010  0.000407  0.000006  \n",
       "3       0.000067  0.000092  0.000081  0.000615  \n",
       "4       0.003676  0.009849  0.114888  0.698732  \n",
       "...          ...       ...       ...       ...  \n",
       "144363  0.490272  0.033270  0.019173  0.042754  \n",
       "144364  0.000881  0.066321  0.000128  0.000100  \n",
       "144365  0.000478  0.001215  0.000067  0.000032  \n",
       "144366  0.000020  0.000142  0.000007  0.000004  \n",
       "144367  0.000709  0.046507  0.000115  0.000042  \n",
       "\n",
       "[144368 rows x 10 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
